{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "509cfca4",
   "metadata": {
    "id": "509cfca4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tfDzqKOqsA1i",
   "metadata": {
    "id": "tfDzqKOqsA1i"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('small_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94f57d96",
   "metadata": {
    "id": "94f57d96"
   },
   "outputs": [],
   "source": [
    "df_train, df_val_test, tar_train, tar_val_test = train_test_split(df['clean_motiv_part'].tolist(), df['label'].tolist(), train_size=0.8, random_state=1412) \n",
    "df_test, df_val, tar_test, tar_val = train_test_split(df_val_test, tar_val_test, train_size=0.5, random_state=1412)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3deb887a",
   "metadata": {
    "id": "3deb887a"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "  def __init__(self, texts, targets, tokenizer, max_len=512):\n",
    "    self.texts = texts\n",
    "    self.targets = targets\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    text = str(self.texts[idx])\n",
    "    target = self.targets[idx]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=self.max_len,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      'text': text,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "W1mJirBE6mro",
   "metadata": {
    "id": "W1mJirBE6mro"
   },
   "outputs": [],
   "source": [
    "class BertClassifier:\n",
    "\n",
    "    def __init__(self, model_path, tokenizer_path, n_classes=None, epochs=None, model_save_path='bert_model.pth'):\n",
    "        #self.model = BertForSequenceClassification.from_pretrained(model_path, ignore_mismatched_sizes=True, output_hidden_states=True)\n",
    "        #self.tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "        self.model = torch.load(model_path)\n",
    "        self.tokenizer = torch.load(tokenizer_path)\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.max_len = 512\n",
    "        self.model_save_path=model_save_path\n",
    "        self.epochs = epochs\n",
    "        #self.out_features = self.model.bert.encoder.layer[1].output.dense.out_features\n",
    "        #self.model.classifier = torch.nn.Linear(self.out_features, n_classes)\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "    def preparation(self, X_train, y_train, X_valid, y_valid):\n",
    "        batch_size = 512\n",
    "        # create datasets\n",
    "        self.train_set = CustomDataset(X_train, y_train, self.tokenizer)\n",
    "        self.valid_set = CustomDataset(X_valid, y_valid, self.tokenizer)\n",
    "\n",
    "        # create data loaders\n",
    "        self.train_loader = DataLoader(self.train_set, batch_size=batch_size, shuffle=True)\n",
    "        self.valid_loader = DataLoader(self.valid_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # helpers initialization\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=2e-5, correct_bias=False)\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "                self.optimizer,\n",
    "                num_warmup_steps=0,\n",
    "                num_training_steps=len(self.train_loader) * self.epochs\n",
    "            )\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss().to(self.device)\n",
    "\n",
    "    def fit(self):\n",
    "        self.model = self.model.train()\n",
    "        losses = []\n",
    "        correct_predictions = 0\n",
    "\n",
    "        for data in self.train_loader:\n",
    "            input_ids = data[\"input_ids\"].to(self.device)\n",
    "            attention_mask = data[\"attention_mask\"].to(self.device)\n",
    "            targets = data[\"targets\"].to(self.device)\n",
    "\n",
    "            outputs = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "                )\n",
    "\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            loss = self.loss_fn(outputs.logits, targets)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        train_acc = correct_predictions.double() / len(self.train_set)\n",
    "        train_loss = np.mean(losses)\n",
    "        return train_acc, train_loss\n",
    "    \n",
    "    def eval(self):\n",
    "        with torch.no_grad():\n",
    "            self.model = self.model.eval()\n",
    "            losses = []\n",
    "            correct_predictions = 0\n",
    "            for data in self.valid_loader:\n",
    "                input_ids = data[\"input_ids\"].to(self.device)\n",
    "                attention_mask = data[\"attention_mask\"].to(self.device)\n",
    "                targets = data[\"targets\"].to(self.device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                    )\n",
    "\n",
    "                preds = torch.argmax(outputs.logits, dim=1)\n",
    "                loss = self.loss_fn(outputs.logits, targets)\n",
    "                correct_predictions += torch.sum(preds == targets)\n",
    "                losses.append(loss.item())\n",
    "\n",
    "        val_acc = correct_predictions.double() / len(self.valid_set)\n",
    "        val_loss = np.mean(losses)\n",
    "        return val_acc, val_loss\n",
    "    \n",
    "    def train(self):\n",
    "        best_accuracy = 0\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'Epoch {epoch + 1}/{self.epochs}\\n')\n",
    "            train_acc, train_loss = self.fit()\n",
    "            print(f'Train loss {train_loss} accuracy {train_acc}\\n')\n",
    "\n",
    "            val_acc, val_loss = self.eval()\n",
    "            print(f'Val loss {val_loss} accuracy {val_acc}\\n')\n",
    "            print('--------------------\\n')\n",
    "\n",
    "            if val_acc > best_accuracy:\n",
    "                #torch.save(self.model, self.model_save_path)\n",
    "                best_accuracy = val_acc\n",
    "\n",
    "    def predict(self, text):\n",
    "        if text == np.NaN:\n",
    "            return 0\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        out = {\n",
    "              'text': text,\n",
    "              'input_ids': encoding['input_ids'].flatten(),\n",
    "              'attention_mask': encoding['attention_mask'].flatten()\n",
    "          }\n",
    "\n",
    "        input_ids = out[\"input_ids\"].to(self.device)\n",
    "        attention_mask = out[\"attention_mask\"].to(self.device)\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids.unsqueeze(0),\n",
    "            attention_mask=attention_mask.unsqueeze(0)\n",
    "        )\n",
    "\n",
    "        prediction = torch.argmax(outputs.logits, dim=1).cpu().numpy()[0]\n",
    "\n",
    "        return prediction\n",
    "    \n",
    "    def predict_proba(self, text):\n",
    "        if text == np.NaN:\n",
    "            return 0\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        out = {\n",
    "              'text': text,\n",
    "              'input_ids': encoding['input_ids'].flatten(),\n",
    "              'attention_mask': encoding['attention_mask'].flatten()\n",
    "          }\n",
    "\n",
    "        input_ids = out[\"input_ids\"].to(self.device)\n",
    "        attention_mask = out[\"attention_mask\"].to(self.device)\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids.unsqueeze(0),\n",
    "            attention_mask=attention_mask.unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "        return outputs.logits.cpu().detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9bfe50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"cointegrated/rubert-tiny\", ignore_mismatched_sizes=True, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "rR-ilJeA-Vq0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rR-ilJeA-Vq0",
    "outputId": "7cc3a84c-bdb4-422d-b43d-0279a47bfd0e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ilian\\anaconda3\\envs\\my_env\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_path = 'new_bert_model.pth'\n",
    "tokenizer_path = 'bert_tokenizer.pt'\n",
    "\n",
    "model = BertClassifier(model_path, tokenizer_path, n_classes=206, epochs=50)\n",
    "model.preparation(df_train, tar_train, df_val, tar_val)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7a7fe83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7595/7595 [05:24<00:00, 23.39it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "pred_y = []\n",
    "for i in tqdm(df_test):\n",
    "    pred_y.append(model.predict(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d14c4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8496379196840026"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(tar_test, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8nFTFQ5S4DYT",
   "metadata": {
    "id": "8nFTFQ5S4DYT"
   },
   "outputs": [],
   "source": [
    "def get_emb(word, model, tokenizer, device):\n",
    "    encoding = tokenizer.encode_plus(word, add_special_tokens=False, return_tensors='pt')\n",
    "    out = {\n",
    "            'text': word,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "    input_ids = out[\"input_ids\"].to(device)\n",
    "    attention_mask = out[\"attention_mask\"].to(device)\n",
    "\n",
    "    outputs = model(\n",
    "        input_ids=input_ids.unsqueeze(0),\n",
    "        attention_mask=attention_mask.unsqueeze(0)\n",
    "    )\n",
    "\n",
    "    return outputs.hidden_states[-1].mean(dim=1)[0].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v7Q-ouyPaP4T",
   "metadata": {
    "id": "v7Q-ouyPaP4T"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "emb_3gr = dict()\n",
    "emb_word = dict()\n",
    "\n",
    "bert = model.model\n",
    "tok = model.tokenizer\n",
    "device = model.device\n",
    "bert.eval() \n",
    "with torch.no_grad():\n",
    "    for text in df['clean_motiv_part']:        \n",
    "        list_words = text.split(\" \")\n",
    "        \n",
    "        n, ind = len(list_words) - 2, 0\n",
    "\n",
    "        while not list_words[ind].isalpha() and ind < n:\n",
    "            ind += 1\n",
    "        second_word = list_words[ind]\n",
    "        ind += 1\n",
    "        while not list_words[ind].isalpha() and ind < n:\n",
    "            ind += 1\n",
    "        third_word = list_words[ind]\n",
    "        ind += 1\n",
    "        \n",
    "        second_emb = get_emb(second_word, bert, tok, device)\n",
    "        third_emb = get_emb(third_word, bert, tok, device)\n",
    "        emb_word[second_word] = second_emb\n",
    "        emb_word[third_word] = third_emb\n",
    "\n",
    "        while ind < n:\n",
    "            first_word = second_word\n",
    "            second_word = third_word\n",
    "            while not list_words[ind].isalpha() and ind < n:\n",
    "                ind += 1\n",
    "            third_word = list_words[ind]\n",
    "            \n",
    "            first_emb = second_emb\n",
    "            second_emb = third_emb\n",
    "            third_emb = get_emb(third_word, bert, tok, device)\n",
    "            \n",
    "            emb_word[third_word] = third_emb\n",
    "            emb_3gr[first_word+' '+second_word+' '+third_word] = torch.cat((first_emb, second_emb, third_emb), dim=0)\n",
    "\n",
    "            ind+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ja8FCPK015k1",
   "metadata": {
    "id": "Ja8FCPK015k1"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "a_file = open(\"dict_emb_gr.pkl\", \"wb\")\n",
    "pickle.dump(emb_3gr, a_file)\n",
    "a_file.close()\n",
    "\n",
    "b_file = open(\"dict_emb_word.pkl\", \"wb\")\n",
    "pickle.dump(emb_word, b_file)\n",
    "b_file.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "bert_with_embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
